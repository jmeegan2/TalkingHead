<!DOCTYPE html>
<html>
<head>
  <title>Talking Head - MP3 example</title>
  <style>
    body, html { width:100%; height:100%; max-width: 100%; margin: auto; position: relative; background-color: rgb(0, 0, 0); color: white; overflow: hidden; }
    #avatar { display: block; width:100%; height:100%; position: absolute; z-index: 1; }
    #background { display: block; width:100%; height:100%; position: absolute; z-index: 0; object-fit: cover; }
    #loading { display: block; position: absolute; bottom: 10px; left: 10px; right: 10px; height: 50px; font-family: Arial; font-size: 20px; z-index: 2; }
  </style>

  <script type="importmap">
  { "imports":
    {
      "three": "https://cdn.jsdelivr.net/npm/three@0.161.0/build/three.module.js/+esm",
      "three/addons/": "https://cdn.jsdelivr.net/npm/three@0.161.0/examples/jsm/",
      "talkinghead": "https://cdn.jsdelivr.net/gh/met4citizen/TalkingHead@1.2/modules/talkinghead.mjs"
    }
  }
  </script>

  <script type="module">
    import * as THREE from "three";
    import { TalkingHead } from "talkinghead";

    let head; // TalkingHead instance
    let audio; // Audio object

    async function loadAudio(file) {
      try {
        // OpenAI Whisper request
        const form = new FormData();
        form.append("file", file);
        form.append("model", "whisper-1");
        form.append("language", "en");
        form.append("response_format", "verbose_json");
        form.append("timestamp_granularities[]", "word");
        form.append("timestamp_granularities[]", "segment");

        const response = await fetch("https://api.openai.com/v1/audio/transcriptions", {
          method: "POST",
          body: form,
          headers: {
            "Authorization": "Bearer <add-your-openai-api-key-here>" // <- Change this
          }
        });

        if (response.ok) {
          const json = await response.json();

          // Fetch audio
          if (json.words && json.words.length) {
            var reader = new FileReader();
            reader.readAsArrayBuffer(file);

            reader.onload = async readerEvent => {
              let arraybuffer = readerEvent.target.result;
              let audiobuffer = await head.audioCtx.decodeAudioData(arraybuffer);

              // TalkingHead audio object
              audio = {
                audio: audiobuffer,
                words: [],
                wtimes: [],
                wdurations: [],
                markers: [],
                mtimes: []
              };

              // Add words to the audio object
              json.words.forEach(x => {
                audio.words.push(x.word);
                audio.wtimes.push(1000 * x.start - 150);
                audio.wdurations.push(1000 * (x.end - x.start));
              });

              // Callback function to make the avatar look at the camera
              const startSegment = async () => {
                head.lookAtCamera(500);
                head.speakWithHands();
              };

              // Add timed callback markers to the audio object
              json.segments.forEach(x => {
                if (x.start > 2 && x.text.length > 10) {
                  audio.markers.push(startSegment);
                  audio.mtimes.push(1000 * x.start - 1000);
                }
              });

              head.speakAudio(audio);
            }
          }
        } else {
          console.error('Error:', response.status, response.statusText);
        }
      } catch (error) {
        console.error(error);
      }
    }

    document.addEventListener('DOMContentLoaded', async function(e) {
      // Instantiate the class
      const nodeAvatar = document.getElementById('avatar');
      head = new TalkingHead(nodeAvatar, {
        ttsEndpoint: "https://eu-texttospeech.googleapis.com/v1beta1/text:synthesize",
        lipsyncModules: ["en", "fi"],
        cameraView: "full"
      });

      // Load and show the avatar
      const nodeLoading = document.getElementById('loading');
      try {
        await head.showAvatar({
          url: 'https://models.readyplayer.me/669db2b58409082e90d840e5.glb?morphTargets=ARKit,Oculus+Visemes,mouthOpen,mouthSmile,eyesClosed,eyesLookUp,eyesLookDown&textureSizeLimit=1024&textureFormat=png',
          body: 'M',
          avatarMood: 'neutral',
          ttsLang: "en-GB",
          ttsVoice: "en-GB-Standard-A",
          lipsyncLang: 'en'
        }, (ev) => {
          if (ev.lengthComputable) {
            let val = Math.min(100, Math.round(ev.loaded / ev.total * 100));
            nodeLoading.textContent = "Loading " + val + "%";
          }
        });
        nodeLoading.style.display = 'none';
      } catch (error) {
        console.error(error);
        nodeLoading.textContent = error.toString();
      }

      // Add keydown event listener for space key
      document.addEventListener('keydown', (event) => {
        if (event.code === 'Space') {
          listen();
        }
      });
    });

    let waitingForResponse = false;

    window.listen = function() {
      if (waitingForResponse) return;
      console.log('listen');

      const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
      recognition.lang = 'en-US';
      recognition.interimResults = false;
      recognition.maxAlternatives = 1;

      recognition.start();

      recognition.onresult = (event) => {
        const speechResult = event.results[0][0].transcript;
        console.log('Result: ', speechResult);
        waitingForResponse = true;
        callServer(speechResult);
      };

      recognition.onspeechend = () => recognition.stop();
      recognition.onerror = (event) => console.error('Error occurred in recognition: ', event.error);
    };

    async function callServer(speechResult) {
      try {
        const response = await fetch('http://localhost:3000/chat', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ text: speechResult }),
        });

        if (!response.ok) throw new Error('Network response was not ok');

        // Create a Blob from the response
        const blob = await response.blob();

        // Create a File from the Blob
        const file = new File([blob], 'response.mp3', { type: 'audio/mpeg' });

        // Call loadAudio with the created file
        await loadAudio(file);

        waitingForResponse = false;
      } catch (error) {
        console.error('Fetch error for chatgpt', error);
        waitingForResponse = false;
      }
    }
  </script>
</head>

<body>
  <img id="background" src="/TalkingHead/views/retro-gaming-geek-room.jpg" alt="Background Image" />


  <div id="avatar"></div>
  <div id="loading"></div>
</body>
</html>
